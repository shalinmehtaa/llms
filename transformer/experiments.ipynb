{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842705b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterable\n",
    "from typing import Optional\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae152d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(torch.optim.Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"] # Get the learning rate\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p] # Get state associated with p.\n",
    "                t = state.get(\"t\", 0) # Get iteration number from the state, or initial value.\n",
    "                grad = p.grad.data # Get the gradient of loss with respect to p.\n",
    "                p.data -= lr / math.sqrt(t + 1) * grad # Update weight tensor in-place.\n",
    "                state[\"t\"] = t + 1 # Increment iteration number.\n",
    "                return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f82945c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = SGD([weights], lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4669cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.470273971557617\n",
      "22.540849685668945\n",
      "21.907804489135742\n",
      "21.404787063598633\n",
      "20.978832244873047\n",
      "20.6052303314209\n",
      "20.270122528076172\n",
      "19.964824676513672\n",
      "19.6834774017334\n",
      "19.421907424926758\n",
      "19.177013397216797\n",
      "18.946426391601562\n",
      "18.728282928466797\n",
      "18.521089553833008\n",
      "18.323619842529297\n",
      "18.13486099243164\n",
      "17.953968048095703\n",
      "17.780208587646484\n",
      "17.612972259521484\n",
      "17.45171546936035\n",
      "17.295970916748047\n",
      "17.14533042907715\n",
      "16.99942398071289\n",
      "16.857934951782227\n",
      "16.72057342529297\n",
      "16.587074279785156\n",
      "16.457210540771484\n",
      "16.330766677856445\n",
      "16.207550048828125\n",
      "16.087387084960938\n",
      "15.970115661621094\n",
      "15.855589866638184\n",
      "15.743671417236328\n",
      "15.634238243103027\n",
      "15.527173042297363\n",
      "15.422367095947266\n",
      "15.319723129272461\n",
      "15.219145774841309\n",
      "15.120550155639648\n",
      "15.023857116699219\n",
      "14.92898941040039\n",
      "14.8358736038208\n",
      "14.74444580078125\n",
      "14.654644012451172\n",
      "14.566405296325684\n",
      "14.4796781539917\n",
      "14.39440631866455\n",
      "14.31054401397705\n",
      "14.228041648864746\n",
      "14.146855354309082\n",
      "14.06694221496582\n",
      "13.988260269165039\n",
      "13.910776138305664\n",
      "13.834449768066406\n",
      "13.759246826171875\n",
      "13.685133934020996\n",
      "13.612081527709961\n",
      "13.540058135986328\n",
      "13.469037055969238\n",
      "13.39898681640625\n",
      "13.329883575439453\n",
      "13.261702537536621\n",
      "13.194418907165527\n",
      "13.128008842468262\n",
      "13.062450408935547\n",
      "12.997723579406738\n",
      "12.933806419372559\n",
      "12.870678901672363\n",
      "12.80832290649414\n",
      "12.746718406677246\n",
      "12.685851097106934\n",
      "12.625700950622559\n",
      "12.566252708435059\n",
      "12.507490158081055\n",
      "12.4493989944458\n",
      "12.39196491241455\n",
      "12.335171699523926\n",
      "12.279006004333496\n",
      "12.223457336425781\n",
      "12.168508529663086\n",
      "12.114150047302246\n",
      "12.060369491577148\n",
      "12.007155418395996\n",
      "11.95449447631836\n",
      "11.90237808227539\n",
      "11.850793838500977\n",
      "11.799734115600586\n",
      "11.749184608459473\n",
      "11.699140548706055\n",
      "11.649587631225586\n",
      "11.600521087646484\n",
      "11.551929473876953\n",
      "11.503803253173828\n",
      "11.456138610839844\n",
      "11.408923149108887\n",
      "11.36214828491211\n",
      "11.315810203552246\n",
      "11.26990032196045\n",
      "11.224409103393555\n",
      "11.179329872131348\n"
     ]
    }
   ],
   "source": [
    "for t in range(100):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b0d93f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.134657859802246\n",
      "7.126181125640869\n",
      "5.253116607666016\n",
      "4.110002517700195\n",
      "3.329101800918579\n",
      "2.760206937789917\n",
      "2.3278684616088867\n",
      "1.9892299175262451\n",
      "1.7178564071655273\n",
      "1.496443748474121\n"
     ]
    }
   ],
   "source": [
    "opt = SGD([weights], lr=10)\n",
    "for t in range(10):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3516ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3131428956985474\n",
      "0.840411365032196\n",
      "0.6195153594017029\n",
      "0.4847046136856079\n",
      "0.3926107883453369\n",
      "0.32551929354667664\n",
      "0.27453234791755676\n",
      "0.23459571599960327\n",
      "0.2025918811559677\n",
      "0.1764800250530243\n",
      "0.15486279129981995\n",
      "0.1367487758398056\n",
      "0.12141421437263489\n",
      "0.10831809788942337\n",
      "0.09704789519309998\n",
      "0.08728361874818802\n",
      "0.07877346873283386\n",
      "0.0713166669011116\n",
      "0.06475135684013367\n",
      "0.0589456781744957\n"
     ]
    }
   ],
   "source": [
    "opt = SGD([weights], lr=10)\n",
    "for t in range(20):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762950b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05379130691289902\n",
      "0.00860660895705223\n",
      "0.002852849429473281\n",
      "0.001218679128214717\n",
      "0.0005971528007648885\n",
      "0.0003196819743607193\n",
      "0.00018225137318950146\n",
      "0.00010896284220507368\n",
      "6.763714918633923e-05\n",
      "4.328777504269965e-05\n"
     ]
    }
   ],
   "source": [
    "opt = SGD([weights], lr=30)\n",
    "for t in range(10):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31b1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
